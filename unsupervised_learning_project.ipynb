{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-12fa6f36cf01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmplot3d\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAxes3D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "# Load libraries and initialize values\n",
    "\n",
    "\n",
    "# seed value for random number generators to obtain reproducible results\n",
    "RANDOM_SEED = 1\n",
    "\n",
    "\n",
    "# although we standardize X and y variables on input,\n",
    "# we will fit the intercept term in the models\n",
    "# Expect fitted values to be close to zero\n",
    "SET_FIT_INTERCEPT = True\n",
    "\n",
    "# import base packages into the namespace for this program\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# modeling routines from Scikit Learn packages\n",
    "\n",
    "# Assignment 4: Be sure to add decision tree and random forest packages\n",
    "\n",
    "from sklearn.datasets import fetch_mldata, fetch_openml\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, fbeta_score\n",
    "from sklearn.metrics import matthews_corrcoef, brier_score_loss\n",
    "# from sklearn.metrics import roc_curve, roc_auc_score  # Not relevant to a multiclass classifier\n",
    "# from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, StandardScaler, RobustScaler\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "import sklearn.linear_model\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LassoLarsIC\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.metrics import f1_score, mean_squared_error, r2_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "# Let's comment out the math library. Stick with the numpy version of the function.\n",
    "\n",
    "# from math import sqrt  # log function; sqrt for root mean-squared error calculation\n",
    "\n",
    "# np.sqrt works on vectors and thus plays well with matplotlib\n",
    "# Attempts to transform more than single numbers with math.sqrt often crash plt plots\n",
    "# Speaking of plt… Here are libraries for visualizations\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import graphviz\n",
    "import pydot\n",
    "\n",
    "# Sanity check: Echo output so that we know that this critical first chunk has run successfully\n",
    "print(\"All libraries imported. RANDOM_SEED = {:d}\".format(RANDOM_SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with a small housekeeping task: Creating a decorator function for tracking time. This code, alas, cannot distinguish between calls to the same function by different parts of the program. However, having a count of time spent on certain repeated tasks will help us diagnose which parts of the random forest and PCA-dimension reduction tasks consume the most time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time decorator code adapted from:\n",
    "#   https://stackoverflow.com/questions/3620943/measuring-elapsed-time-with-the-time-module\n",
    "# Key changes:\n",
    "#   time.monotonic() instead of time.time()\n",
    "#   print_prof_data() function revamped to show function / calls / total / average / max in tabular format\n",
    "\n",
    "PROF_DATA = {}\n",
    "\n",
    "def profile(fn):\n",
    "    @wraps(fn)\n",
    "    def with_profiling(*args, **kwargs):\n",
    "        start_time = time.monotonic()\n",
    "        \n",
    "        ret = fn(*args, **kwargs)\n",
    "        \n",
    "        elapsed_time = time.monotonic() - start_time\n",
    "\n",
    "        if fn.__name__ not in PROF_DATA:\n",
    "            PROF_DATA[fn.__name__] = [0, []]\n",
    "        PROF_DATA[fn.__name__][0] += 1\n",
    "        PROF_DATA[fn.__name__][1].append(elapsed_time)\n",
    "\n",
    "        return ret\n",
    "\n",
    "    return with_profiling\n",
    "\n",
    "def print_prof_data():\n",
    "    print(\"\\n{:>25}\\t{:>8}\\t{:>12}\\t{:>12}\\t{:>12}\\t{:>12}\".format(\"Function\", \"Calls\", \"Total\", \"Average\", \"Std dev\", \"Maximum\"))\n",
    "    print(108 * \"_\" + \"\\n\")\n",
    "    total_calls = 0\n",
    "    grand_time = 0\n",
    "    for fname, data in PROF_DATA.items():\n",
    "        total_calls += data[0]\n",
    "        total_time = sum(data[1])\n",
    "        grand_time += total_time\n",
    "        avg_time = np.mean(data[1])\n",
    "        std_time = np.std(data[1])\n",
    "        max_time = max(data[1])\n",
    "        print(\"{:>25s}\\t{:8d}\\t{:12.6f}\\t{:12.6f}\\t{:12.6f}\\t{:12.6f}\".format(fname, data[0], total_time, avg_time, std_time, max_time))\n",
    "    print(108 * \"_\")\n",
    "    print(\"\\n{:>25}\\t{:8d}\\t{:>12.6f}\\t{:12.6f}\".format(\"ALL FUNCTIONS COMBINED\", total_calls, grand_time, grand_time / total_calls))\n",
    "\n",
    "def clear_prof_data():\n",
    "    global PROF_DATA\n",
    "    PROF_DATA = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the time decorator function — Set up\n",
    "\n",
    "@profile\n",
    "def factorial(n):\n",
    "    \"\"\"Assumes n is an integer > 0\n",
    "        Returns n!\"\"\"\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    elif n == 1:\n",
    "        return n\n",
    "    else:\n",
    "        return n * factorial(n - 1)\n",
    "\n",
    "@profile\n",
    "def permutations(n, r):\n",
    "    \"\"\" Assumes n, k are integers > 0\n",
    "        Returns nPr \"\"\"\n",
    "    nPr = factorial(n)/factorial(n - r)\n",
    "    nPr = int(round(nPr))\n",
    "    return nPr\n",
    "\n",
    "@profile\n",
    "def combinations(n, r):\n",
    "    \"\"\" Assumes n, k are integers > 0\n",
    "        Returns nCr \"\"\"\n",
    "    nCr = permutations(n, r)/factorial(r)\n",
    "    nCr = int(round(nCr))\n",
    "    return nCr\n",
    "\n",
    "# Test the time decorator function — Write a tiny Fibonnaci triangle\n",
    "\n",
    "for i in range(0, 13):\n",
    "    for j in range(i + 1):\n",
    "        print(combinations(i, j), \"\\t\", end = \"\")\n",
    "        # print(\"C({:d}, {:d}) = {:d}\\t\".format(i, j, combinations(i, j)), end = \"\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "print_prof_data()\n",
    "\n",
    "print(\"\\nNow clearing the timekeeping cache so that we keep accurate track of machine-learning tasks …\")\n",
    "clear_prof_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's begin our machine-learning tasks in earnest!\n",
    "# Begin by loading the full set of 784 explanatory variables in the MNIST data set\n",
    "\n",
    "# This appears to be deprecated code, but it shows up in MSDS program and Géron code\n",
    "# mnist = fetch_openml('MNIST original')\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version = 1)\n",
    "# mnist  # This echoes the MNIST dataset\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "\n",
    "# Recast y as integers\n",
    "X = X.astype(np.uint8)\n",
    "y = y.astype(np.uint8)\n",
    "\n",
    "print(\"The shape of mnist X (features):\\t\", X.shape)\n",
    "print(\"The shape of mnist y (target):  \\t\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first entry in mnist\n",
    "\n",
    "pick_a_digit = 4\n",
    "\n",
    "print(\"This should be a\", y[pick_a_digit])\n",
    "some_digit = X[pick_a_digit]\n",
    "some_digit_image = some_digit.reshape(28, 28)\n",
    "plt.imshow(some_digit_image, cmap = mpl.cm.binary, interpolation = \"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"some_digit_plot.png\")\n",
    "plt.savefig(\"some_digit_plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration code for showing off a set of mnist images\n",
    "\n",
    "def plot_digits(instances, images_per_row=10, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary, **options)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "example_images = np.r_[X[:12000:600], X[13000:30600:600], X[30600:60000:590]]\n",
    "plot_digits(example_images, images_per_row=10)\n",
    "plt.savefig(\"more_digits_plot.png\")\n",
    "plt.savefig(\"more_digits_plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having loaded MNIST, we will now split that dataset into training and test sets. We will also define our primary scorer as one based on the F1 score. Finally, we will frontload numerous scoring, reporting, and plotting functions so that their designation does _not_ count against the time recorded for each of the machine-learning routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split mnist into training and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "\n",
    "# This is an exceptionally important line! We need an F1 scorer in place of \"accuracy\"\n",
    "\n",
    "f1_scorer = make_scorer(f1_score, average = \"micro\", greater_is_better = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval_frame = pd.DataFrame(np.zeros((5, 5), dtype = float),\n",
    "                              columns = [\"random_forest\", \"flawed_PCA\", \"correct_PCA\", \"PCA_pipe\", \"sgd\"],\n",
    "                              index = [\"f1_score\", \"standard_deviation\", \"score_0\", \"score_1\", \"score_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A defined function for Kfold cross-validation: Use 3 folds in light of the heightened computational expense\n",
    "\n",
    "np.set_printoptions(precision = 6)\n",
    "CV_FOLDS = 3\n",
    "\n",
    "@profile\n",
    "def crossval_scoring(model, label, column_name, features = X_train):\n",
    "    print(\"{:>30}\\t{:>10}\\t{:>8}\\t{:>18}\".format(\"Multiclass classifier\", \"F1 score\", \"Std Dev\", \"Scores\"))\n",
    "    print(92 * \"_\")\n",
    "    score_column = np.zeros(5, dtype = float)\n",
    "    scores = cross_val_score(model, features, y_train, scoring = \"f1_micro\", cv = CV_FOLDS)\n",
    "    score_column[0] = scores.mean()\n",
    "    score_column[1] = scores.std()\n",
    "    score_column[2] = scores[0]\n",
    "    score_column[3] = scores[1]\n",
    "    score_column[4] = scores[2]\n",
    "    crossval_frame[column_name] = score_column\n",
    "    print(\"{:>30}\\t{:10.6f}\\t{:8.6f}\\t{}\".format(label, scores.mean(), scores.std(), scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some ways to display confusion matrixes graphically\n",
    "# Leave the routines adapted from SciKit-Learn documentation and MSDS sample code for future use\n",
    "# The \"purple_haze\" routine combines the best of both worlds. We'll stick with that version. See bottom.\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, standardize = False,\n",
    "                          title = None, file_name = \"default\", cmap = plt.cm.Blues):\n",
    "    \n",
    "    # This code enables the display of the confusion matrix in graphic format\n",
    "    # Code adapted from SciKit-Learn documentation:\n",
    "    # https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \n",
    "    if not title:\n",
    "        if standardize:\n",
    "            title = \"Standardized confusion matrix\"\n",
    "        else:\n",
    "            title = \"Unstandardized confusion matrix\"\n",
    "    title += \"\\n\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if standardize:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis = 1)[:, np.newaxis]\n",
    "        \n",
    "    # Sklearn documentation offers to print the confusion matrix. Pass for now.\n",
    "    # print(title)\n",
    "    # print(cm)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = (9, 9))\n",
    "    im = ax.imshow(cm, interpolation = \"nearest\", cmap = cmap)\n",
    "    ax.figure.colorbar(im, ax = ax)\n",
    "    ax.set(xticks = np.arange(cm.shape[1]), yticks = np.arange(cm.shape[0]),\n",
    "           xticklabels = classes, yticklabels = classes, title = title,\n",
    "           ylabel = \"True label\\n\", xlabel = \"\\nPredicted label\")\n",
    "    plt.setp(ax.get_xticklabels(), rotation = 0, ha = \"right\",\n",
    "             rotation_mode = \"anchor\")\n",
    "    fmt = \".3f\" if standardize else \"d\"\n",
    "    thresh = cm.max() / 2 if not standardize else 0.5\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt), ha = \"center\", va = \"center\",\n",
    "                    color = \"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(file_name + \"_confmat.png\")\n",
    "    plt.savefig(file_name + \"_confmat.pdf\")\n",
    "    return ax\n",
    "\n",
    "def colorful_confusion_matrix(matrix, title = \"Colorful confusion matrix\", file_name = \"default\"):\n",
    "    fig = plt.figure(figsize=(9, 9))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set(title = title + \"\\n\", ylabel = \"True label\\n\", xlabel = \"\\nPredicted label\")\n",
    "    cax = ax.matshow(matrix)\n",
    "    fig.colorbar(cax)\n",
    "    plt.savefig(file_name + \"_colorCM.png\")\n",
    "    plt.savefig(file_name + \"_colorCM.pdf\")\n",
    "\n",
    "def standardized_confusion_matrix(matrix):\n",
    "    row_sums = matrix.sum(axis = 1, keepdims = True)\n",
    "    standardized_matrix = matrix / row_sums\n",
    "    np.fill_diagonal(standardized_matrix, 0)\n",
    "    return standardized_matrix\n",
    "\n",
    "@profile\n",
    "def purple_haze(y_true, y_pred, classes, standardize = False, title = None, file_name = \"default\",\n",
    "                cmap = plt.cm.plasma):\n",
    "    \n",
    "    # This code enables the display of the confusion matrix in graphic format\n",
    "    # Code adapted from SciKit-Learn documentation:\n",
    "    # https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \n",
    "    if not title:\n",
    "        if standardize:\n",
    "            title = \"Standardized confusion matrix\"\n",
    "        else:\n",
    "            title = \"Unstandardized confusion matrix\"\n",
    "    title += \"\\n\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if standardize:\n",
    "        np.fill_diagonal(cm, 0)  # Add this line; see what happens\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis = 1)[:, np.newaxis]\n",
    "        \n",
    "    # Sklearn documentation offers to print the confusion matrix. Pass for now.\n",
    "    # print(title)\n",
    "    # print(cm)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = (9, 9))\n",
    "    im = ax.imshow(cm, interpolation = \"nearest\", cmap = cmap)\n",
    "    ax.figure.colorbar(im, ax = ax)\n",
    "    ax.set(xticks = np.arange(cm.shape[1]), yticks = np.arange(cm.shape[0]),\n",
    "           xticklabels = classes, yticklabels = classes, title = title,\n",
    "           ylabel = \"True label\\n\", xlabel = \"\\nPredicted label\")\n",
    "    plt.setp(ax.get_xticklabels(), rotation = 0, ha = \"right\",\n",
    "             rotation_mode = \"anchor\")\n",
    "    fmt = \".3f\" if standardize else \"d\"\n",
    "    thresh = cm.max() / 2 # if not standardize else 0.4\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt), ha = \"center\", va = \"center\",\n",
    "                    color = \"black\" if cm[i, j] > thresh else \"white\")\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(file_name + \"_confmat.png\")\n",
    "    plt.savefig(file_name + \"_confmat.pdf\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scorecard_frame in anticipation of scoring functions\n",
    "# We'll be glad in the final cells that we took the trouble to do this now\n",
    "\n",
    "scorecard_frame = pd.DataFrame(np.zeros((9, 5), dtype = float),\n",
    "                              columns = [\"random_forest\", \"flawed_PCA\", \"correct_PCA\", \"PCA_pipe\", \"sgd\"],\n",
    "                              index = [\"training_score\", \"test_score\", \"precision\", \"recall\", \"f1_score\",\n",
    "                                       \"f2_score\", \"specificity\", \"bookmaker_informedness\",\n",
    "                                       \"matthews_correlation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring functions\n",
    "\n",
    "def specificity_score(y_true, y_pred):\n",
    "    ss = confusion_matrix(y_true, y_pred)[0, 0] / sum(confusion_matrix(y_true, y_pred)[0])\n",
    "    return ss\n",
    "\n",
    "def bookmaker_informedness(y_true, y_pred):\n",
    "    bm = recall_score(y_true, y_pred, average = None) + specificity_score(y_true, y_pred) - 1\n",
    "    return bm\n",
    "\n",
    "@profile\n",
    "def scorecard(model, train, test, column_name):\n",
    "    fitted_model = model.fit(train, y_train)\n",
    "    model_name = str(model).partition(\"(\")[0] + \" | \" + column_name\n",
    "    model_predict = model.predict(test)\n",
    "    score_column = np.zeros(9, dtype = float)\n",
    "    print(\"Classification scorecard for \" + model_name + \":\\n\")\n",
    "    \n",
    "    train_score = model.score(train, y_train)\n",
    "    score_column[0] = train_score\n",
    "    print(\"{:>25}\\t{:.6f}\".format(\"Training set score\", train_score))\n",
    "    \n",
    "    test_score = model.score(test, y_test)\n",
    "    score_column[1] = test_score\n",
    "    print(\"{:>25}\\t{:.6f}\".format(\"Test set score\", test_score))\n",
    "    \n",
    "    precision = precision_score(y_test, model_predict, average = None).mean()\n",
    "    score_column[2] = precision\n",
    "    print(\"{:>25}\\t{:.6f}\".format(\"Precision\", precision))\n",
    "    \n",
    "    recall = recall_score(y_test, model_predict, average = None).mean()\n",
    "    score_column[3] = recall\n",
    "    print(\"{:>25}\\t{:.6f}\".format(\"Recall\", recall))\n",
    "    \n",
    "    f1 = f1_score(y_test, model_predict, average = None).mean()\n",
    "    score_column[4] = f1\n",
    "    print(\"{:>25}\\t{:.6f}\".format(\"F1 score\", f1))\n",
    "    \n",
    "    f2 = fbeta_score(y_test, model_predict, beta = 2, average = None).mean()\n",
    "    score_column[5] = f2\n",
    "    print(\"{:>25}\\t{:.6f}\".format(\"F2 score\", f2))\n",
    "    \n",
    "    specificity = specificity_score(y_test, model_predict)\n",
    "    score_column[6] = specificity\n",
    "    print(\"{:>25}\\t{:.6f}\".format(\"Specificity\", specificity))\n",
    "    \n",
    "    bmi = bookmaker_informedness(y_test, model_predict).mean()\n",
    "    score_column[7] = bmi\n",
    "    print(\"{:>25}\\t{:.6f}\".format(\"Bookmaker informedness\", bmi))\n",
    "    \n",
    "    matt_corr = matthews_corrcoef(y_test, model_predict)\n",
    "    score_column[8] = matt_corr\n",
    "    print(\"{:>25}\\t{:.6f}\".format(\"Matthews correlation\", matt_corr))\n",
    "    \n",
    "    scorecard_frame[column_name] = score_column\n",
    "\n",
    "# The following code was the original code for the scorecard\n",
    "# Keep it around — It worked (very) well. Its only \"flaw\" was not recording its output for a dataframe\n",
    "\n",
    "def scorecard_backup(model, train, test):\n",
    "    fitted_model = model.fit(train, y_train)\n",
    "    model_name = str(model).partition(\"(\")[0]\n",
    "    model_predict = model.predict(test)\n",
    "    print(\"Classification scorecard for \" + model_name + \":\\n\")\n",
    "    print(\"{:>25}\\t{:.6f}\".format(\"Training set score\", model.score(train, y_train)))\n",
    "    print(\"{:>25}\\t{:.6f}\".format(\"Test set score\", model.score(test, y_test)))\n",
    "    print(\"{:>25}\\t{:.6f}\".format(\"Precision\", precision_score(y_test, model_predict, average = None).mean()))\n",
    "    print(\"{:>25}\\t{:.6f}\".format(\"Recall\", recall_score(y_test, model_predict, average = None).mean()))\n",
    "    print(\"{:>25}\\t{:.6f}\".format(\"F1 score\", f1_score(y_test, model_predict, average = None).mean()))\n",
    "    print(\"{:>25}\\t{:.6f}\".format(\"F2 score\", fbeta_score(y_test, model_predict, beta = 2, average = None).mean()))\n",
    "    print(\"{:>25}\\t{:.6f}\".format(\"Specificity\", specificity_score(y_test, model_predict)))\n",
    "    print(\"{:>25}\\t{:.6f}\".format(\"Bookmaker informedness\", bookmaker_informedness(y_test, model_predict).mean()))\n",
    "    print(\"{:>25}\\t{:.6f}\".format(\"Matthews correlation\", matthews_corrcoef(y_test, model_predict)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the clock on the original random forest process\n",
    "\n",
    "RF_start_time = time.monotonic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load and test a Random Forest Classifier!\n",
    "\n",
    "# Use GridSearchCV() to evaluate random forest classifers\n",
    "\n",
    "@profile\n",
    "def GridSearchForest(features, target):\n",
    "    feature_count = len(features[0])\n",
    "    low_depth = int(round(np.log2(feature_count)))\n",
    "    mid_depth = int(round(np.sqrt(2) * np.log2(feature_count)))\n",
    "    high_depth = int(round(2 * np.log2(feature_count)))\n",
    "    low_features = int(round(np.sqrt(feature_count)))\n",
    "    mid_features = int(round(np.sqrt(np.e) * np.sqrt(feature_count)))\n",
    "    high_features = int(round(np.e * np.sqrt(feature_count)))\n",
    "    param_grid = {\"max_depth\": [low_depth, mid_depth, high_depth],\n",
    "                  \"max_features\": [low_features, mid_features, high_features]}\n",
    "    grid_search_forest = GridSearchCV(RandomForestClassifier(random_state = RANDOM_SEED, n_estimators = 20),\n",
    "                                      param_grid, cv = 3, return_train_score = True, scoring = f1_scorer, iid = False)\n",
    "    grid_search_forest.fit(features, target)\n",
    "    forest_depth = grid_search_forest.best_params_[\"max_depth\"]\n",
    "    forest_features = grid_search_forest.best_params_[\"max_features\"]\n",
    "    forest_score = grid_search_forest.best_score_\n",
    "    forest_grid_cv_results = grid_search_forest.cv_results_\n",
    "    return forest_depth, forest_features, forest_score, forest_grid_cv_results\n",
    "\n",
    "forest_depth, forest_features, forest_score, forest_grid_cv_results = GridSearchForest(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Maximum depth:   \\t{:d}\".format(forest_depth))\n",
    "print(\"Maximum features:\\t{:d}\".format(forest_features))\n",
    "print(\"Best cv score:   \\t{:.6f}\".format(forest_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture the GridSearch results in a dataframe and display them\n",
    "\n",
    "forest_grid_search_results = pd.DataFrame(forest_grid_cv_results)\n",
    "\n",
    "forest_grid_search_results.rename(columns = {\"param_max_depth\": \"max_depth\",\n",
    "                                             \"param_max_features\": \"max_features\"}, inplace = True)\n",
    "forest_grid_search_results[\"total_time\"] = forest_grid_search_results[\"mean_fit_time\"] + forest_grid_search_results[\"mean_score_time\"]\n",
    "forest_grid_search_results[\"standardized_time\"] = forest_grid_search_results.total_time.map(lambda x: x / sum(forest_grid_search_results.total_time))\n",
    "forest_grid_search_results.loc[:, [\"rank_test_score\", \"max_depth\", \"max_features\", \"mean_test_score\", \"mean_train_score\", \"mean_fit_time\", \"mean_score_time\", \"total_time\", \"standardized_time\"]].sort_values(by = \"rank_test_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the optimization of the basic Random Forest Classifier in 3D\n",
    "\n",
    "fig = plt.figure(figsize = (10, 10))\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "plot3d_title = \"Random Forest Classifier cross-validation via GridSearch\\n\"\n",
    "plot3d_title += \"F1 score ≈ \" + str(round(forest_score, 6))\n",
    "plot3d_title += \" at max_depth = \" + str(forest_depth) + \" and max_features = \" + str(forest_features)\n",
    "\n",
    "ax.set_title(plot3d_title)\n",
    "ax.set_xlabel(\"max_depth\", color = \"r\")\n",
    "ax.set_ylabel(\"max_features\", color = \"b\")\n",
    "ax.set_zlabel(\"F1 score\", color = \"g\")\n",
    "\n",
    "ax.plot(forest_grid_search_results[\"max_depth\"], forest_grid_search_results[\"max_features\"],\n",
    "           forest_grid_search_results[\"mean_test_score\"], color = \"blue\", label = \"Hyperparameter testing path\")\n",
    "\n",
    "ax.scatter(forest_depth, forest_features, forest_score, s = 256, marker = \"o\",\n",
    "           color = \"#ff0000cc\", label = \"Optimal depth, features\")\n",
    "\n",
    "depth_space =    np.linspace(forest_grid_search_results[\"max_depth\"].min(),\n",
    "                             forest_grid_search_results[\"max_depth\"].max() + 1, 120)\n",
    "features_space = np.linspace(forest_grid_search_results[\"max_features\"].min(),\n",
    "                             forest_grid_search_results[\"max_features\"].max() + 1, 120)\n",
    "\n",
    "plot_x, plot_y = np.meshgrid(depth_space, features_space)\n",
    "score_plane = 0 * plot_x + 0 * plot_y + forest_score\n",
    "\n",
    "ax.plot_wireframe(plot_x, plot_y, score_plane, rcount = 6, ccount = 6, color = \"#00884466\")\n",
    "\n",
    "ax.legend(loc = \"lower right\")\n",
    "ax.view_init(12, 45)\n",
    "plt.savefig(\"forest_hyperparameters.png\")\n",
    "plt.savefig(\"forest_hyperparameters.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the optimization of the basic Random Forest Classifier in 3D — Tracking time\n",
    "\n",
    "fig = plt.figure(figsize = (10, 10))\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "ax.set_xlabel(\"\\nProcessing time (standardized)\", color = \"r\")\n",
    "ax.set_ylabel(\"\\nTrain minus test ~ Overfitting\", color = \"b\")\n",
    "ax.set_zlabel(\"\\nF1 score\", color = \"g\")\n",
    "\n",
    "ax.plot(forest_grid_search_results[\"standardized_time\"],\n",
    "        forest_grid_search_results[\"mean_train_score\"] - forest_grid_search_results[\"mean_test_score\"],\n",
    "        forest_grid_search_results[\"mean_test_score\"], color = \"blue\", label = \"Hyperparameter testing path\")\n",
    "\n",
    "winner_index = forest_grid_search_results[\"mean_test_score\"].idxmax()\n",
    "\n",
    "x_locate = forest_grid_search_results.standardized_time[winner_index]\n",
    "y_locate = forest_grid_search_results.mean_train_score[winner_index] - forest_grid_search_results.mean_test_score[winner_index]\n",
    "\n",
    "ax.scatter(x_locate, y_locate, forest_score, s = 256, marker = \"o\", color = \"#ff0000cc\",\n",
    "           label = \"Optimal depth, features\")\n",
    "\n",
    "depth_space =    np.linspace(forest_grid_search_results[\"standardized_time\"].min(),\n",
    "                             forest_grid_search_results[\"standardized_time\"].max(), 120)\n",
    "features_space = np.linspace(min(forest_grid_search_results[\"mean_train_score\"] - forest_grid_search_results[\"mean_test_score\"]),\n",
    "                             max(forest_grid_search_results[\"mean_train_score\"] - forest_grid_search_results[\"mean_test_score\"]), 120)\n",
    "\n",
    "plot3d_title = \"Random Forest Classifier cross-validation | \"\n",
    "plot3d_title += \"F1 score ≈ \" + str(round(forest_score, 6))\n",
    "plot3d_title +=  \"\\nStandardized time ≈ \" + str(round(x_locate, 6))\n",
    "plot3d_title += \" | Train minus test ≈ \" + str(round(y_locate, 6))\n",
    "ax.set_title(plot3d_title)\n",
    "\n",
    "plot_x, plot_y = np.meshgrid(depth_space, features_space)\n",
    "score_plane = 0 * plot_x + 0 * plot_y + forest_score\n",
    "\n",
    "ax.plot_wireframe(plot_x, plot_y, score_plane, rcount = 8, ccount = 7, color = \"#00884466\")\n",
    "\n",
    "ax.legend(loc = \"lower right\")\n",
    "ax.view_init(12, 45)\n",
    "plt.savefig(\"forest_hyperparameters_time.png\")\n",
    "plt.savefig(\"forest_hyperparameters_time.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a random forest classifier based on hyperparameters reported by GridSearch\n",
    "\n",
    "@profile\n",
    "def forest_fit(features, target):\n",
    "    forest = RandomForestClassifier(random_state = RANDOM_SEED, max_depth = forest_depth,\n",
    "                                    max_features = forest_features, n_estimators = 20)\n",
    "    fit = forest.fit(features, target)\n",
    "    return fit\n",
    "\n",
    "forest = forest_fit(X_train, y_train)\n",
    "\n",
    "# Forgo the printing of these scores — They take time, and the information appears in the scorecard\n",
    "# print(\"Training set score:\\t{:.6f}\".format(forest.score(X_train, y_train)))\n",
    "# print(\"Test set score:\\t\\t{:6f}\".format(forest.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval_scoring(forest, \"Random forest classifier\", \"random_forest\", X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorecard(forest, X_train, X_test, \"random_forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish with a flourish!\n",
    "# Plot confusion matrixes — ordinary and standardized — for the random forest classifier\n",
    "# \"Purple Haze\" is an allusion to a Jimi Hendrix song. Haze = confusion. Purple = plasma or viridis colormap\n",
    "\n",
    "purple_haze(y_test, forest.predict(X_test), classes = forest.classes_,\n",
    "            title = \"Random forest classifier\", file_name = \"forest\")\n",
    "\n",
    "# Use this cell to retain function calls using alternative methods of expressing the confusion matrix\n",
    "\n",
    "# plot_confusion_matrix(y_test, forest.predict(X_test), classes = forest.classes_,\n",
    "#                       title = \"Random forest classifier\", file_name = \"forest\")\n",
    "\n",
    "# plot_confusion_matrix(y_test, forest.predict(X_test), classes = forest.classes_, standardize = True,\n",
    "#                       title = \"Random forest classifier — Standardized\", file_name = \"forest_standardized\")\n",
    "\n",
    "# Colorful confusion matrix, via MSDS sample code\n",
    "# This image requires you to define the matrix, and then call the function\n",
    "\n",
    "# RF_cm = confusion_matrix(y_test, forest.predict(X_test))\n",
    "# colorful_confusion_matrix(RF_cm, \"Random forest\", \"forest\")\n",
    "\n",
    "# RF_standard_cm = standardized_confusion_matrix(RF_cm)\n",
    "# colorful_confusion_matrix(RF_standard_cm, \"Random forest — Standardized\", \"forest_standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purple_haze(y_test, forest.predict(X_test), classes = forest.classes_, standardize = True,\n",
    "            title = \"Random forest classifier — Standardized\", file_name = \"forest_standardized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the random forest clock\n",
    "\n",
    "RF_elapsed_time = time.monotonic() - RF_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions unique to the PCA processes\n",
    "# These functions will be \"wrapped\" for timing purposes …\n",
    "# But the definition of functions themselves won't count against either process\n",
    "\n",
    "@profile\n",
    "def PCA_report(variance_ratio, subtitle = \"\"):\n",
    "    plot_title = \"Reducing MNIST to \" + str(len(variance_ratio)) + \" principal components,\\n\"\n",
    "    plot_title += \"which account for \" + str(round(sum(variance_ratio) * 100, 2)) + \"% of the variance\"\n",
    "    plt.grid()\n",
    "    plt.title(plot_title)\n",
    "    plt.plot(np.cumsum(variance_ratio), color = \"blue\", linestyle = \"-\", linewidth = 1,\n",
    "             label = \"Cumulative sum of explained variance\")\n",
    "    plt.axhline(0.95, color = \"red\", linestyle = \"--\", linewidth = 1, label = \"95% cumulative variance\")\n",
    "    plt.ylabel(\"Explained variance\")\n",
    "    plt.xlabel(\"Principal components\")\n",
    "    plt.legend(loc = \"best\")\n",
    "    plt.savefig(subtitle + \"PCA_cumsum.png\")\n",
    "    plt.savefig(subtitle + \"PCA_cumsum.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the clock on the flawed PCA process\n",
    "\n",
    "reduced_start_time = time.monotonic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def PCA_reduce(features):\n",
    "    pca = PCA(n_components = 0.95)\n",
    "    reduced = pca.fit_transform(features)\n",
    "    variance_ratio = pca.explained_variance_ratio_\n",
    "    return reduced, variance_ratio\n",
    "\n",
    "X_reduced, X_variance_ratio = PCA_reduce(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_report(X_variance_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced_train = X_reduced[:60000]\n",
    "X_reduced_test = X_reduced[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced_forest_features, reduced_forest_score = GridSearchForest(X_reduced_train, y_train)\n",
    "reduced_forest_depth, reduced_forest_features, reduced_forest_score, reduced_forest_grid_cv_results = GridSearchForest(X_reduced_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Maximum features:\\t{:.6f}\".format(reduced_forest_features))\n",
    "# print(\"Best cv score:   \\t{:.6f}\".format(reduced_forest_score))\n",
    "\n",
    "print(\"Maximum depth:   \\t{:d}\".format(reduced_forest_depth))\n",
    "print(\"Maximum features:\\t{:d}\".format(reduced_forest_features))\n",
    "# print(\"Number of trees: \\t{:d}\".format(reduced_forest_estimators))\n",
    "print(\"Best cv score:   \\t{:.6f}\".format(reduced_forest_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture the GridSearch results in a dataframe and display them\n",
    "\n",
    "reduced_forest_grid_search_results = pd.DataFrame(reduced_forest_grid_cv_results)\n",
    "\n",
    "reduced_forest_grid_search_results.rename(columns = {\"param_max_depth\": \"max_depth\",\n",
    "                                             \"param_max_features\": \"max_features\"}, inplace = True)\n",
    "reduced_forest_grid_search_results[\"total_time\"] = reduced_forest_grid_search_results[\"mean_fit_time\"] + reduced_forest_grid_search_results[\"mean_score_time\"]\n",
    "reduced_forest_grid_search_results[\"standardized_time\"] = reduced_forest_grid_search_results.total_time.map(lambda x: x / sum(reduced_forest_grid_search_results.total_time))\n",
    "reduced_forest_grid_search_results.loc[:, [\"rank_test_score\", \"max_depth\", \"max_features\", \"mean_test_score\", \"mean_train_score\", \"mean_fit_time\", \"mean_score_time\", \"total_time\", \"standardized_time\"]].sort_values(by = \"rank_test_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the optimization of the flawed PCA transformation in 3D\n",
    "\n",
    "fig = plt.figure(figsize = (10, 10))\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "plot3d_title = \"(Flawed) PCA-transformed random forest cross-validation via GridSearch\\n\"\n",
    "plot3d_title += \"F1 score ≈ \" + str(round(reduced_forest_score, 6))\n",
    "plot3d_title += \" at max_depth = \" + str(reduced_forest_depth) + \" and max_features = \" + str(reduced_forest_features)\n",
    "\n",
    "ax.set_title(plot3d_title)\n",
    "ax.set_xlabel(\"max_depth\", color = \"r\")\n",
    "ax.set_ylabel(\"max_features\", color = \"b\")\n",
    "ax.set_zlabel(\"F1 score\", color = \"g\")\n",
    "\n",
    "ax.plot(reduced_forest_grid_search_results[\"max_depth\"], reduced_forest_grid_search_results[\"max_features\"],\n",
    "           reduced_forest_grid_search_results[\"mean_test_score\"], color = \"blue\", label = \"Hyperparameter testing path\")\n",
    "\n",
    "ax.scatter(reduced_forest_depth, reduced_forest_features, reduced_forest_score, s = 256, marker = \"o\",\n",
    "           color = \"#ff0000cc\", label = \"Optimal depth, features\")\n",
    "\n",
    "depth_space =    np.linspace(reduced_forest_grid_search_results[\"max_depth\"].min(),\n",
    "                             reduced_forest_grid_search_results[\"max_depth\"].max() + 1, 120)\n",
    "features_space = np.linspace(reduced_forest_grid_search_results[\"max_features\"].min(),\n",
    "                             reduced_forest_grid_search_results[\"max_features\"].max() + 1, 120)\n",
    "\n",
    "plot_x, plot_y = np.meshgrid(depth_space, features_space)\n",
    "score_plane = 0 * plot_x + 0 * plot_y + reduced_forest_score\n",
    "\n",
    "ax.plot_wireframe(plot_x, plot_y, score_plane, rcount = 6, ccount = 6, color = \"#00884466\")\n",
    "\n",
    "ax.legend(loc = \"lower right\")\n",
    "ax.view_init(12, 45)\n",
    "plt.savefig(\"reduced_hyperparameters.png\")\n",
    "plt.savefig(\"reduced_hyperparameters.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the optimization of the flawed PCA-reduced Random Forest Classifier in 3D — Tracking time\n",
    "\n",
    "fig = plt.figure(figsize = (10, 10))\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "ax.set_xlabel(\"\\nProcessing time (standardized)\", color = \"r\")\n",
    "ax.set_ylabel(\"\\nTrain minus test ~ Overfitting\", color = \"b\")\n",
    "ax.set_zlabel(\"\\nF1 score\", color = \"g\")\n",
    "\n",
    "ax.plot(reduced_forest_grid_search_results[\"standardized_time\"],\n",
    "        reduced_forest_grid_search_results[\"mean_train_score\"] - reduced_forest_grid_search_results[\"mean_test_score\"],\n",
    "        reduced_forest_grid_search_results[\"mean_test_score\"], color = \"blue\", label = \"Hyperparameter testing path\")\n",
    "\n",
    "winner_index = reduced_forest_grid_search_results[\"mean_test_score\"].idxmax()\n",
    "\n",
    "x_locate = reduced_forest_grid_search_results.standardized_time[winner_index]\n",
    "y_locate = reduced_forest_grid_search_results.mean_train_score[winner_index] - reduced_forest_grid_search_results.mean_test_score[winner_index]\n",
    "\n",
    "ax.scatter(x_locate, y_locate, reduced_forest_score, s = 256, marker = \"o\", color = \"#ff0000cc\",\n",
    "           label = \"Optimal depth, features\")\n",
    "\n",
    "depth_space =    np.linspace(reduced_forest_grid_search_results[\"standardized_time\"].min(),\n",
    "                             reduced_forest_grid_search_results[\"standardized_time\"].max(), 120)\n",
    "features_space = np.linspace(min(reduced_forest_grid_search_results[\"mean_train_score\"] - reduced_forest_grid_search_results[\"mean_test_score\"]),\n",
    "                             max(reduced_forest_grid_search_results[\"mean_train_score\"] - reduced_forest_grid_search_results[\"mean_test_score\"]), 120)\n",
    "\n",
    "plot3d_title = \"(Improperly) reduced random forest cross-validation | \"\n",
    "plot3d_title += \"F1 score ≈ \" + str(round(reduced_forest_score, 6))\n",
    "plot3d_title +=  \"\\nStandardized time ≈ \" + str(round(x_locate, 6))\n",
    "plot3d_title += \" | Train minus test ≈ \" + str(round(y_locate, 6))\n",
    "ax.set_title(plot3d_title)\n",
    "\n",
    "plot_x, plot_y = np.meshgrid(depth_space, features_space)\n",
    "score_plane = 0 * plot_x + 0 * plot_y + reduced_forest_score\n",
    "\n",
    "ax.plot_wireframe(plot_x, plot_y, score_plane, rcount = 8, ccount = 7, color = \"#00884466\")\n",
    "\n",
    "ax.legend(loc = \"lower right\")\n",
    "ax.view_init(12, 45)\n",
    "plt.savefig(\"reduced_hyperparameters_time.png\")\n",
    "plt.savefig(\"reduced_hyperparameters_time.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a random forest classifier based on hyperparameters reported by GridSearch\n",
    "\n",
    "@profile\n",
    "def reduced_forest_fit(features, target, mdepth, mfeatures):\n",
    "    forest = RandomForestClassifier(random_state = RANDOM_SEED, max_depth = mdepth, max_features = mfeatures,\n",
    "                                    n_estimators = 20)\n",
    "    fit = forest.fit(features, target)\n",
    "    return fit\n",
    "\n",
    "reduced_forest = reduced_forest_fit(X_reduced_train, y_train, reduced_forest_depth, reduced_forest_features)\n",
    "\n",
    "# Forgo these lines to save time — The information will appear in the scorecard\n",
    "# print(\"Training set score:\\t{:.6f}\".format(reduced_forest.score(X_reduced_train, y_train)))\n",
    "# print(\"Test set score:\\t\\t{:6f}\".format(reduced_forest.score(X_reduced_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval_scoring(reduced_forest, \"(Flawed) PCA transformation\", \"flawed_PCA\", X_reduced_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorecard(reduced_forest, X_reduced_train, X_reduced_test, \"flawed_PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purple_haze(y_test, reduced_forest.predict(X_reduced_test), classes = reduced_forest.classes_,\n",
    "            title = \"Reduced random forest classifier\", file_name = \"reduced_forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purple_haze(y_test, reduced_forest.predict(X_reduced_test), classes = reduced_forest.classes_, standardize = True,\n",
    "            title = \"Reduced random forest classifier — Standardized\", file_name = \"reduced_forest_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the clock on the flawed PCA transformation\n",
    "\n",
    "reduced_elapsed_time = time.monotonic() - reduced_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the clock on the flawed PCA process\n",
    "\n",
    "PCA_start_time = time.monotonic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beyond the \"design flaw\" — Let's redo PCA reduction, the right way\n",
    "# We have to limit PCA transformation to the test set\n",
    "# Then use linear algebra to recenter the test set with the eigenvectors of the PCA-transformed train set\n",
    "# Details: https://stats.stackexchange.com/questions/55718/pca-and-the-train-test-split\n",
    "\n",
    "@profile\n",
    "def PCA_train_test(train, test):\n",
    "    pca = PCA(n_components = 0.95)\n",
    "    PCA_train = pca.fit_transform(train)\n",
    "    PCA_test = pca.transform(test)  # Absolutely vital that the test set be transformed, but *not* fit here\n",
    "    variance_ratio = pca.explained_variance_ratio_\n",
    "    return PCA_train, PCA_test, variance_ratio\n",
    "\n",
    "X_PCA_train, X_PCA_test, X_PCA_variance_ratio = PCA_train_test(X_train, X_test)\n",
    "\n",
    "print(\"Proper PCA-reduction of MNIST completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_report(X_PCA_variance_ratio, subtitle = \"correct_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA_forest_features, PCA_forest_score = GridSearchForest(X_PCA_train, y_train)\n",
    "\n",
    "PCA_forest_depth, PCA_forest_features, PCA_forest_score, PCA_forest_grid_cv_results = GridSearchForest(X_PCA_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Maximum features:\\t{:d}\".format(PCA_forest_features))\n",
    "# print(\"Best cv score:   \\t{:.6f}\".format(PCA_forest_score))\n",
    "\n",
    "print(\"Maximum depth:   \\t{:d}\".format(PCA_forest_depth))\n",
    "print(\"Maximum features:\\t{:d}\".format(PCA_forest_features))\n",
    "print(\"Best cv score:   \\t{:.6f}\".format(PCA_forest_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture the GridSearch results in a dataframe and display them\n",
    "\n",
    "PCA_forest_grid_search_results = pd.DataFrame(PCA_forest_grid_cv_results)\n",
    "\n",
    "PCA_forest_grid_search_results.rename(columns = {\"param_max_depth\": \"max_depth\",\n",
    "                                             \"param_max_features\": \"max_features\"}, inplace = True)\n",
    "PCA_forest_grid_search_results[\"total_time\"] = PCA_forest_grid_search_results[\"mean_fit_time\"] + PCA_forest_grid_search_results[\"mean_score_time\"]\n",
    "PCA_forest_grid_search_results[\"standardized_time\"] = PCA_forest_grid_search_results.total_time.map(lambda x: x / sum(PCA_forest_grid_search_results.total_time))\n",
    "PCA_forest_grid_search_results.loc[:, [\"rank_test_score\", \"max_depth\", \"max_features\", \"mean_test_score\", \"mean_train_score\", \"mean_fit_time\", \"mean_score_time\", \"total_time\", \"standardized_time\"]].sort_values(by = \"rank_test_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the optimization of the correct PCA transformation in 3D\n",
    "\n",
    "fig = plt.figure(figsize = (10, 10))\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "plot3d_title = \"*Correct* PCA-transformed Random Forest cross-validation via GridSearch\\n\"\n",
    "plot3d_title += \"F1 score ≈ \" + str(round(PCA_forest_score, 6))\n",
    "plot3d_title += \" at max_depth = \" + str(PCA_forest_depth) + \" and max_features = \" + str(PCA_forest_features)\n",
    "\n",
    "ax.set_title(plot3d_title)\n",
    "ax.set_xlabel(\"max_depth\", color = \"r\")\n",
    "ax.set_ylabel(\"max_features\", color = \"b\")\n",
    "ax.set_zlabel(\"F1 score\", color = \"g\")\n",
    "\n",
    "ax.plot(PCA_forest_grid_search_results[\"max_depth\"], PCA_forest_grid_search_results[\"max_features\"],\n",
    "           PCA_forest_grid_search_results[\"mean_test_score\"], color = \"blue\", label = \"Hyperparameter testing path\")\n",
    "\n",
    "ax.scatter(PCA_forest_depth, PCA_forest_features, PCA_forest_score, s = 256, marker = \"o\",\n",
    "           color = \"#ff0000cc\", label = \"Optimal depth, features\")\n",
    "\n",
    "depth_space =    np.linspace(PCA_forest_grid_search_results[\"max_depth\"].min(),\n",
    "                             PCA_forest_grid_search_results[\"max_depth\"].max() + 1, 120)\n",
    "features_space = np.linspace(PCA_forest_grid_search_results[\"max_features\"].min(),\n",
    "                             PCA_forest_grid_search_results[\"max_features\"].max() + 1, 120)\n",
    "\n",
    "plot_x, plot_y = np.meshgrid(depth_space, features_space)\n",
    "score_plane = 0 * plot_x + 0 * plot_y + PCA_forest_score\n",
    "\n",
    "ax.plot_wireframe(plot_x, plot_y, score_plane, rcount = 6, ccount = 6, color = \"#00884466\")\n",
    "\n",
    "ax.legend(loc = \"lower right\")\n",
    "ax.view_init(12, 45)\n",
    "plt.savefig(\"PCA_hyperparameters.png\")\n",
    "plt.savefig(\"PCA_hyperparameters.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the optimization of the basic Random Forest Classifier in 3D — Tracking time\n",
    "\n",
    "fig = plt.figure(figsize = (10, 10))\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "ax.set_xlabel(\"\\nProcessing time (standardized)\", color = \"r\")\n",
    "ax.set_ylabel(\"\\nTrain minus test ~ Overfitting\", color = \"b\")\n",
    "ax.set_zlabel(\"\\nF1 score\", color = \"g\")\n",
    "\n",
    "ax.plot(PCA_forest_grid_search_results[\"standardized_time\"],\n",
    "        PCA_forest_grid_search_results[\"mean_train_score\"] - PCA_forest_grid_search_results[\"mean_test_score\"],\n",
    "        PCA_forest_grid_search_results[\"mean_test_score\"], color = \"blue\", label = \"Hyperparameter testing path\")\n",
    "\n",
    "winner_index = PCA_forest_grid_search_results[\"mean_test_score\"].idxmax()\n",
    "\n",
    "x_locate = PCA_forest_grid_search_results.standardized_time[winner_index]\n",
    "y_locate = PCA_forest_grid_search_results.mean_train_score[winner_index] - PCA_forest_grid_search_results.mean_test_score[winner_index]\n",
    "\n",
    "ax.scatter(x_locate, y_locate, PCA_forest_score, s = 256, marker = \"o\", color = \"#ff0000cc\",\n",
    "           label = \"Optimal depth, features\")\n",
    "\n",
    "depth_space =    np.linspace(PCA_forest_grid_search_results[\"standardized_time\"].min(),\n",
    "                             PCA_forest_grid_search_results[\"standardized_time\"].max(), 120)\n",
    "features_space = np.linspace(min(PCA_forest_grid_search_results[\"mean_train_score\"] - PCA_forest_grid_search_results[\"mean_test_score\"]),\n",
    "                             max(PCA_forest_grid_search_results[\"mean_train_score\"] - PCA_forest_grid_search_results[\"mean_test_score\"]), 120)\n",
    "\n",
    "plot3d_title = \"*Correctly* reduced random forest cross-validation | \"\n",
    "plot3d_title += \"F1 score ≈ \" + str(round(PCA_forest_score, 6))\n",
    "plot3d_title +=  \"\\nStandardized time ≈ \" + str(round(x_locate, 6))\n",
    "plot3d_title += \" | Train minus test ≈ \" + str(round(y_locate, 6))\n",
    "ax.set_title(plot3d_title)\n",
    "\n",
    "plot_x, plot_y = np.meshgrid(depth_space, features_space)\n",
    "score_plane = 0 * plot_x + 0 * plot_y + PCA_forest_score\n",
    "\n",
    "ax.plot_wireframe(plot_x, plot_y, score_plane, rcount = 8, ccount = 7, color = \"#00884466\")\n",
    "\n",
    "ax.legend(loc = \"lower right\")\n",
    "ax.view_init(12, 45)\n",
    "plt.savefig(\"PCA_hyperparameters_time.png\")\n",
    "plt.savefig(\"PCA_hyperparameters_time.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA_forest = reduced_forest_fit(X_PCA_train, y_train, PCA_forest_features)\n",
    "# print(\"Training set score:\\t{:.6f}\".format(PCA_forest.score(X_PCA_train, y_train)))\n",
    "# print(\"Test set score:\\t\\t{:6f}\".format(PCA_forest.score(X_PCA_test, y_test)))\n",
    "\n",
    "PCA_forest = reduced_forest_fit(X_PCA_train, y_train, PCA_forest_depth, PCA_forest_features)\n",
    "\n",
    "# Forgo these outputs in anticipation of the scorecard\n",
    "# print(\"Training set score:\\t{:.6f}\".format(PCA_forest.score(X_PCA_train, y_train)))\n",
    "# print(\"Test set score:\\t\\t{:6f}\".format(PCA_forest.score(X_PCA_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval_scoring(PCA_forest, \"*Correct* PCA transformation\", \"correct_PCA\", X_PCA_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorecard(PCA_forest, X_PCA_train, X_PCA_test, \"correct_PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purple_haze(y_test, PCA_forest.predict(X_PCA_test), classes = PCA_forest.classes_,\n",
    "            title = \"PCA random forest classifier\", file_name = \"PCA_forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purple_haze(y_test, PCA_forest.predict(X_PCA_test), classes = PCA_forest.classes_, standardize = True,\n",
    "            title = \"PCA random forest classifier — Standardized\", file_name = \"PCA_forest_standardized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the clock on the correct PCA transformation\n",
    "\n",
    "PCA_elapsed_time = time.monotonic() - PCA_start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Bonus material*\n",
    "\n",
    "Having run our random forest classifier and _two_ versions of its 95% transformation through PCA (one flawed and the other correct), we may now turn our attention to alternative classifiers. We start with a stochastic gradient descent classifier (SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a stochastic gradient descent (SGD) classifier\n",
    "# Start the clock on SGD\n",
    "\n",
    "SGD_start_time = time.monotonic()\n",
    "\n",
    "# Binary classifier via sample code — Too simple for this exercise\n",
    "# y_train_9 = (y_train == 9)  # True for 9's, false for every other\n",
    "# y_test_9  = (y_test == 9)\n",
    "\n",
    "\n",
    "sgd = SGDClassifier(random_state = RANDOM_SEED)\n",
    "# sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval_scoring(sgd, \"Stochastic gradient descent\", \"sgd\", X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorecard(sgd, X_train, X_test, \"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purple_haze(y_test, sgd.predict(X_test), classes = sgd.classes_,\n",
    "            title = \"Stochastic gradient descent\", file_name = \"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purple_haze(y_test, sgd.predict(X_test), classes = sgd.classes_, standardize = True, \n",
    "            title = \"Stochastic gradient descent — Standardized\", file_name = \"sgd_standardized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the clock on Stochastic Gradient Descent\n",
    "\n",
    "SGD_elapsed_time = time.monotonic() - SGD_start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to complete our analysis by taking three \"final steps.\" We will keep a running clock on these processes as well.\n",
    "\n",
    "1: First, we wish to demonstrate a simple technique for ensuring that PCA analysis is properly conducted. Putting SciKit-Learn's Pipeline() to its original use, wholly apart from GridSearchCV(), allows us to _correctly_ reduce MNIST data through PCA _and_ to fit a PCA-ready version of our streamlined random forest model optimized for 154 features, all in a single step. We will use the cross-validation and scorecard routines developed earlier in this code and report its results in a final dataframe reporting all cross-validation results and and performance metrics.\n",
    "\n",
    "2: Although the task of illustrating some difficult portions of our best performing random forest engine is not trivial, we think it's worth an extra few minutes to show a visual confusion matrix illustrating 4's and 9's that were correctly identified, as well as instances of each of these digits that were confused for the other.\n",
    "\n",
    "3: Finally, we will produce a Final Pipeline comparing the performance and resource-consumption effects of PCA-dimension reduction as well as _scaling_ on random forest models. Scaling has almost no impact on random forest models. However, it has dramatic positive — and negative — effects on stochastic gradient descent. One simply should not generalize the impact of preprocessing on one class of machine-learning models (such as SGD and its linear cousins) onto another class (such as random forests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the clock on final steps, including error analysis of 4-9 confusion\n",
    "# And, of course, we want to lay a *** Final Pipeline ***\n",
    "\n",
    "final_steps_start_time = time.monotonic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we fit and score a simple Pipeline that combines *proper* PCA reduction with our PCA-optimized model\n",
    "\n",
    "PCA_pipe = Pipeline([(\"reduction\", PCA(n_components = 0.95)), (\"model\", PCA_forest)])\n",
    "PCA_pipe.fit(X_train, y_train)\n",
    "PCA_pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will score the PCA_pipe, directly on X_train instead of the a separately fit and transformed data set\n",
    "\n",
    "crossval_scoring(PCA_pipe, \"PCA Pipeline\", \"PCA_pipe\", X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same process is repeated for the performance metric scorecard\n",
    "# The final dataframes for the crossval and performance metric scorecards will reflect all results\n",
    "\n",
    "scorecard(PCA_pipe, X_train, X_test, \"PCA_pipe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis — This is time counted against the \"Final Steps\" tally\n",
    "# But we will profile it with the decorator function so that we know how much time it consumes\n",
    "\n",
    "@profile\n",
    "def error_analysis(cl_a = 4, cl_b = 9):\n",
    "\n",
    "    # cl_a, cl_b = 4, 9\n",
    "    y_train_pred = cross_val_predict(forest, X_train, y_train, cv=3)\n",
    "    X_aa = X_train[(y_train == cl_a) & (y_train_pred == cl_a)]\n",
    "    X_ab = X_train[(y_train == cl_a) & (y_train_pred == cl_b)]\n",
    "    X_ba = X_train[(y_train == cl_b) & (y_train_pred == cl_a)]\n",
    "    X_bb = X_train[(y_train == cl_b) & (y_train_pred == cl_b)]\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(221); plot_digits(X_aa[:25], images_per_row=5)\n",
    "    plt.subplot(222); plot_digits(X_ab[:25], images_per_row=5)\n",
    "    plt.subplot(223); plot_digits(X_ba[:25], images_per_row=5)\n",
    "    plt.subplot(224); plot_digits(X_bb[:25], images_per_row=5)\n",
    "    plt.savefig(\"error_analysis_digits_plot.png\")\n",
    "    plt.savefig(\"error_analysis_digits_plot.pdf\")\n",
    "\n",
    "# If time becomes an issue, we can comment out the function call\n",
    "error_analysis(4, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def preprocessing_pipeline(features = X_train, target = y_train):\n",
    "    \n",
    "    forest_pipe = Pipeline([(\"reduction\", PCA(n_components = 0.95)), (\"scaling\", StandardScaler()),\n",
    "                            (\"model\", RandomForestClassifier())])\n",
    "\n",
    "    param_grid = [{\"model\": [forest], \"reduction\": [None], \"scaling\": [None, StandardScaler(), Normalizer(), MinMaxScaler(), RobustScaler()]},\n",
    "                  {\"model\": [PCA_forest], \"reduction\": [PCA(n_components = 0.95)], \"scaling\": [None, StandardScaler(), Normalizer(), MinMaxScaler(), RobustScaler()]},\n",
    "                  {\"model\": [sgd], \"reduction\": [None], \"scaling\": [None, StandardScaler(), Normalizer(), MinMaxScaler(), RobustScaler()]}\n",
    "                 ]\n",
    "\n",
    "    grid_search_forest_pipe = GridSearchCV(forest_pipe, param_grid, cv = 3, return_train_score = True,\n",
    "                                           scoring = f1_scorer, iid = False)\n",
    "\n",
    "    grid_search_forest_pipe.fit(features, target)\n",
    "    pipe_params  = grid_search_forest_pipe.best_params_\n",
    "    pipe_score   = grid_search_forest_pipe.best_score_\n",
    "    pipe_results = grid_search_forest_pipe.cv_results_\n",
    "    \n",
    "    return pipe_params, pipe_score, pipe_results\n",
    "\n",
    "pipe_params, pipe_score, pipe_results = preprocessing_pipeline(X_train, y_train)\n",
    "\n",
    "print(\"Results of the standard scaling Pipeline:\\n\")\n",
    "print(\"Best parameter:\\t{}\".format(pipe_params))\n",
    "print(\"Best cv score:\\t{:.6f}\".format(pipe_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe_results = grid_search_forest_pipe.cv_results_\n",
    "pipe_frame = pd.DataFrame(pipe_results)\n",
    "pipe_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_frame[\"param_model\"] = pipe_frame[\"param_model\"].map(lambda x: str(x))\n",
    "for record_no in range(len(pipe_frame.param_model)):\n",
    "    model_name = pipe_frame.loc[record_no, \"param_model\"]\n",
    "    if model_name.partition(\"(\")[0] == \"SGDClassifier\":\n",
    "        pipe_frame.loc[record_no, \"param_model\"] = \"SGD\"\n",
    "    elif str.find(model_name, str(forest_features)) > 0:  # A regular random forest model\n",
    "        pipe_frame.loc[record_no, \"param_model\"] = \"RandomForest\"\n",
    "    else:\n",
    "        pipe_frame.loc[record_no, \"param_model\"] = \"PCA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_frame.rename(columns = {\"param_model\": \"model\", \"param_reduction\": \"reduction\", \"param_scaling\": \"scaling\",\n",
    "                             \"mean_test_score\": \"test_score\", \"mean_train_score\": \"train_score\",\n",
    "                             \"mean_fit_time\": \"fit_time\", \"mean_score_time\": \"score_time\"}, inplace = True)\n",
    "pipe_frame[\"scaling\"] = pipe_frame[\"scaling\"].map(lambda x: str(x).partition(\"(\")[0])\n",
    "pipe_frame[\"train_minus_test\"] = pipe_frame[\"train_score\"] - pipe_frame[\"test_score\"]\n",
    "pipe_frame[\"total_time\"] = pipe_frame[\"fit_time\"] + pipe_frame[\"score_time\"]\n",
    "pipe_frame[\"standardized_time\"] = pipe_frame[\"total_time\"].map(lambda x: x / pipe_frame.total_time.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pipe = pipe_frame.loc[:, [\"rank_test_score\", \"model\", \"scaling\", \"test_score\", \"train_score\", \"train_minus_test\", \"fit_time\", \"score_time\", \"total_time\", \"standardized_time\"]].sort_values(by = \"rank_test_score\")\n",
    "final_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Pipeline() in 3D — Test score as a function of processing time and overfitting\n",
    "\n",
    "fig = plt.figure(figsize = (12, 12))\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "ax.set_xlabel(\"\\nProcessing time (standardized)\", color = \"r\")\n",
    "ax.set_ylabel(\"\\nTrain minus test ~ Overfitting\", color = \"b\")\n",
    "ax.set_zlabel(\"\\nF1 score\", color = \"g\")\n",
    "\n",
    "winner_index = final_pipe[\"test_score\"].idxmax()\n",
    "winning_model = final_pipe.loc[winner_index, \"model\"]\n",
    "if final_pipe.loc[winner_index, \"scaling\"] == None:\n",
    "    winning_scaling = \"None\"\n",
    "else:\n",
    "    winning_scaling = final_pipe.loc[winner_index, \"scaling\"]\n",
    "\n",
    "winning_label = \"Best model: \" + winning_model + \" | Scaling: \" + winning_scaling\n",
    "\n",
    "def plot_model_lines(model_name, model_color):\n",
    "    ax.plot(final_pipe[final_pipe.model == model_name][\"standardized_time\"],\n",
    "        final_pipe[final_pipe.model == model_name][\"train_minus_test\"],\n",
    "        final_pipe[final_pipe.model == model_name][\"test_score\"],\n",
    "        color = model_color, label = model_name)\n",
    "\n",
    "def scatter_model_dots(model_name, model_color, model_marker):\n",
    "    ax.scatter(final_pipe[final_pipe.model == model_name][\"standardized_time\"],\n",
    "               final_pipe[final_pipe.model == model_name][\"train_minus_test\"],\n",
    "               final_pipe[final_pipe.model == model_name][\"test_score\"],\n",
    "               s = 96, marker = model_marker, color = model_color, label = model_name)\n",
    "    \n",
    "scatter_model_dots(\"RandomForest\", \"red\", \"o\")\n",
    "scatter_model_dots(\"PCA\", \"blue\", \"^\")\n",
    "plot_model_lines(\"SGD\", \"brown\")\n",
    "\n",
    "x_locate = final_pipe.standardized_time[winner_index]\n",
    "y_locate = final_pipe.train_minus_test[winner_index]\n",
    "\n",
    "# ax.scatter(x_locate, y_locate, pipe_score, s = 256, marker = \"o\", color = \"#ff0000cc\",\n",
    "#            label = winning_label)\n",
    "\n",
    "time_space =    np.linspace(final_pipe[\"standardized_time\"].min(), final_pipe[\"standardized_time\"].max(), 120)\n",
    "overfit_space = np.linspace(final_pipe[\"train_minus_test\"].min(), final_pipe[\"train_minus_test\"].max(), 120)\n",
    "\n",
    "plot3d_title = winning_label + \" | F1 score ≈ \" + str(round(pipe_score, 6))\n",
    "plot3d_title +=  \"\\nStandardized time ≈ \" + str(round(x_locate, 6))\n",
    "plot3d_title += \" | Train minus test ≈ \" + str(round(y_locate, 6))\n",
    "ax.set_title(plot3d_title)\n",
    "\n",
    "plot_x, plot_y = np.meshgrid(time_space, overfit_space)\n",
    "score_plane = 0 * plot_x + 0 * plot_y + pipe_score\n",
    "\n",
    "ax.plot_wireframe(plot_x, plot_y, score_plane, rcount = 8, ccount = 7, color = \"#00884466\")\n",
    "\n",
    "# ax.plot_surface(plot_x, plot_y, score_plane, color = \"#ffff0066\")\n",
    "\n",
    "ax.legend(loc = \"lower right\")\n",
    "ax.view_init(12, 45)\n",
    "plt.savefig(\"final_pipe.png\")\n",
    "plt.savefig(\"final_pipe.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're almost done! Time to harvest all of that pandas work and report their results.\n",
    "# First, the dataframe showing cross-valuation results …\n",
    "\n",
    "crossval_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, the dataframe consolidating results from scorecards on each of our models\n",
    "\n",
    "scorecard_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the clock. This is it.\n",
    "\n",
    "final_steps_elapsed_time = time.monotonic() - final_steps_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:>60}\\n\".format(\"*** FINAL TIMEKEEPING SCORECARD ***\"))\n",
    "\n",
    "print(\"{:>36}\\t{:12.6f}\".format(\"Random forest time\", RF_elapsed_time))\n",
    "print(\"{:>36}\\t{:12.6f}\".format(\"(Flawed) PCA-transformation time\", reduced_elapsed_time))\n",
    "print(\"{:>36}\\t{:12.6f}\".format(\"*Correct* PCA-transformation time\", PCA_elapsed_time))\n",
    "print(\"{:>36}\\t{:12.6f}\".format(\"Stochastic gradient descent time\", SGD_elapsed_time))\n",
    "print(\"{:>36}\\t{:12.6f}\".format(\"Error analysis/Final Pipeline time\", final_steps_elapsed_time))\n",
    "print(\"{:>36}\\t{:12.6f}\\n\".format(\"TOTAL\", RF_elapsed_time + reduced_elapsed_time + PCA_elapsed_time + SGD_elapsed_time + final_steps_elapsed_time))\n",
    "print_prof_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also export PROF_DATA to a DataFrame for nicer display and possibly also for future use\n",
    "# Display the refined dataframe so that …\n",
    "#   1. It suppresses the individual times, without deleting that information\n",
    "#   2. It is sorted in descending order of total time taken\n",
    "\n",
    "profile_frame = pd.DataFrame(PROF_DATA)\n",
    "profile_frame = profile_frame.transpose()\n",
    "profile_frame.columns = [\"calls\", \"times\"]\n",
    "profile_frame[\"total\"] = profile_frame.times.map(sum)\n",
    "profile_frame[\"average\"] = profile_frame.times.map(np.mean)\n",
    "profile_frame[\"std_dev\"] = profile_frame.times.map(np.std)\n",
    "profile_frame[\"maximum\"] = profile_frame.times.map(max)\n",
    "profile_frame[\"minimum\"] = profile_frame.times.map(min)\n",
    "\n",
    "print(\"\\t*** Timekeeping summary of profiled functions ***\\n\")\n",
    "print(\"{:d} profiled functions took {:d} calls for {:.6f} total seconds, an average of {:.6f}.\".format(len(profile_frame), profile_frame.calls.sum(), profile_frame.total.sum(), profile_frame.total.mean()))\n",
    "\n",
    "profile_frame.loc[:, [\"calls\", \"total\", \"average\", \"std_dev\", \"maximum\", \"minimum\"]].sort_values(by = \"total\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
